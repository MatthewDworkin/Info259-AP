{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2-regularized logistic regression for binary or multiclass classification; trains a model (on `train.txt`), optimizes L2 regularization strength on `dev.txt`, and evaluates performance on `test.txt`.  Reports test accuracy with 95% confidence intervals and prints out the strongest coefficients for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TQTT9x-6d2JI"
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from sklearn import linear_model\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import operator\n",
    "import nltk\n",
    "import math\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4KuVSCSqlUX",
    "outputId": "f4cf377b-6d74-473c-a945-828fa09bae92",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/matthewdworkin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols = line.split(\"\\t\")\n",
    "            idd = cols[0]\n",
    "            label = cols[2].lstrip().rstrip()\n",
    "            text = cols[3]\n",
    "\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CGiM8qQiJOBU"
   },
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "\n",
    "    def __init__(self, feature_method, trainX, trainY, devX, devY, testX, testY):\n",
    "        self.feature_vocab = {}\n",
    "        self.feature_method = feature_method\n",
    "        self.min_feature_count=2\n",
    "        self.log_reg = None\n",
    "\n",
    "        self.trainY=trainY\n",
    "        self.devY=devY\n",
    "        self.testY=testY\n",
    "        \n",
    "        self.trainX = self.process(trainX, training=True)\n",
    "        self.devX = self.process(devX, training=False)\n",
    "        self.testX = self.process(testX, training=False)\n",
    "\n",
    "    # Featurize entire dataset\n",
    "    def featurize(self, data):\n",
    "        featurized_data = []\n",
    "        for text in data:\n",
    "            feats = self.feature_method(text)\n",
    "            featurized_data.append(feats)\n",
    "        return featurized_data\n",
    "\n",
    "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
    "    def process(self, X_data, training = False):\n",
    "        \n",
    "        data = self.featurize(X_data)\n",
    "\n",
    "        if training:\n",
    "            fid = 0\n",
    "            feature_doc_count = Counter()\n",
    "            for feats in data:\n",
    "                for feat in feats:\n",
    "                    feature_doc_count[feat]+= 1\n",
    "\n",
    "            for feat in feature_doc_count:\n",
    "                if feature_doc_count[feat] >= self.min_feature_count:\n",
    "                    self.feature_vocab[feat] = fid\n",
    "                    fid += 1\n",
    "\n",
    "        F = len(self.feature_vocab)\n",
    "        D = len(data)\n",
    "        X = sparse.dok_matrix((D, F))\n",
    "        for idx, feats in enumerate(data):\n",
    "            for feat in feats:\n",
    "                if feat in self.feature_vocab:\n",
    "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "    # Train model and evaluate on held-out data\n",
    "    def train(self):\n",
    "        (D,F) = self.trainX.shape\n",
    "        best_dev_accuracy=0\n",
    "        best_model=None\n",
    "        for C in [0.1, 1, 10, 100]:\n",
    "            self.log_reg = linear_model.LogisticRegression(C = C, max_iter=1000)\n",
    "            self.log_reg.fit(self.trainX, self.trainY)\n",
    "            training_accuracy = self.log_reg.score(self.trainX, self.trainY)\n",
    "            development_accuracy = self.log_reg.score(self.devX, self.devY)\n",
    "            if development_accuracy > best_dev_accuracy:\n",
    "                best_dev_accuracy=development_accuracy\n",
    "                best_model=self.log_reg\n",
    "\n",
    "#             print(\"C: %s, Train accuracy: %.3f, Dev accuracy: %.3f\" % (C, training_accuracy, development_accuracy))\n",
    "\n",
    "        self.log_reg=best_model\n",
    "        \n",
    "\n",
    "    def test(self):\n",
    "        return self.log_reg.score(self.testX, self.testY)\n",
    "        \n",
    "\n",
    "    def printWeights(self, n=10):\n",
    "\n",
    "        reverse_vocab=[None]*len(self.log_reg.coef_[0])\n",
    "        for k in self.feature_vocab:\n",
    "            reverse_vocab[self.feature_vocab[k]]=k\n",
    "\n",
    "        # binary\n",
    "        if len(self.log_reg.classes_) == 2:\n",
    "              weights=self.log_reg.coef_[0]\n",
    "\n",
    "              cat=self.log_reg.classes_[1]\n",
    "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
    "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
    "              print()\n",
    "\n",
    "              cat=self.log_reg.classes_[0]\n",
    "              for feature, weight in list(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1)))[:n]:\n",
    "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
    "              print()\n",
    "\n",
    "        # multiclass\n",
    "        else:\n",
    "          for i, cat in enumerate(self.log_reg.classes_):\n",
    "\n",
    "              weights=self.log_reg.coef_[i]\n",
    "\n",
    "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
    "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
    "              print()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_bow_featurize(text):\n",
    "    feats = {}\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    for word in words:\n",
    "        word=word.lower()\n",
    "        feats[word]=1\n",
    "            \n",
    "    return feats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_intervals(accuracy, n, significance_level):\n",
    "    critical_value=(1-significance_level)/2\n",
    "    z_alpha=-1*norm.ppf(critical_value)\n",
    "    se=math.sqrt((accuracy*(1-accuracy))/n)\n",
    "    return accuracy-(se*z_alpha), accuracy+(se*z_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(trainingFile, devFile, testFile):\n",
    "    trainX, trainY=load_data(trainingFile)\n",
    "    devX, devY=load_data(devFile)\n",
    "    testX, testY=load_data(testFile)\n",
    "    \n",
    "    simple_classifier = Classifier(binary_bow_featurize, trainX, trainY, devX, devY, testX, testY)\n",
    "    simple_classifier.train()\n",
    "    accuracy=simple_classifier.test()\n",
    "    \n",
    "    lower, upper=confidence_intervals(accuracy, len(testY), .95)\n",
    "    print(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (accuracy, lower, upper))\n",
    "\n",
    "    simple_classifier.printWeights()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make train test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>adjudicated</th>\n",
       "      <th>humor</th>\n",
       "      <th>woman who teaches special-needs children killing it at dinner party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>adjudicated</td>\n",
       "      <td>satire</td>\n",
       "      <td>dnc criticized for overly restrictive debate r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>adjudicated</td>\n",
       "      <td>irony</td>\n",
       "      <td>custodian taken into custody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>adjudicated</td>\n",
       "      <td>none</td>\n",
       "      <td>mike pence takes oath of office as country's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>adjudicated</td>\n",
       "      <td>humor</td>\n",
       "      <td>bedtime story from fucking bible again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>adjudicated</td>\n",
       "      <td>satire</td>\n",
       "      <td>literary theorists admit they still have no id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>496</td>\n",
       "      <td>adjudicated</td>\n",
       "      <td>none</td>\n",
       "      <td>longing to finally visit the cuba of my dreams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>497</td>\n",
       "      <td>adjudicated</td>\n",
       "      <td>none</td>\n",
       "      <td>trump's evangelical advisors urged him to prot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>498</td>\n",
       "      <td>adjudicated</td>\n",
       "      <td>none</td>\n",
       "      <td>flaws in how we evaluate leaders (from kahnema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>499</td>\n",
       "      <td>adjudicated</td>\n",
       "      <td>irony</td>\n",
       "      <td>intel unveils oversized novelty processor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>500</td>\n",
       "      <td>adjudicated</td>\n",
       "      <td>none</td>\n",
       "      <td>ariana grande singlehandedly saves tidal with ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1  adjudicated   humor  \\\n",
       "0      2  adjudicated  satire   \n",
       "1      3  adjudicated   irony   \n",
       "2      4  adjudicated    none   \n",
       "3      5  adjudicated   humor   \n",
       "4      6  adjudicated  satire   \n",
       "..   ...          ...     ...   \n",
       "494  496  adjudicated    none   \n",
       "495  497  adjudicated    none   \n",
       "496  498  adjudicated    none   \n",
       "497  499  adjudicated   irony   \n",
       "498  500  adjudicated    none   \n",
       "\n",
       "    woman who teaches special-needs children killing it at dinner party  \n",
       "0    dnc criticized for overly restrictive debate r...                   \n",
       "1                         custodian taken into custody                   \n",
       "2    mike pence takes oath of office as country's n...                   \n",
       "3               bedtime story from fucking bible again                   \n",
       "4    literary theorists admit they still have no id...                   \n",
       "..                                                 ...                   \n",
       "494     longing to finally visit the cuba of my dreams                   \n",
       "495  trump's evangelical advisors urged him to prot...                   \n",
       "496  flaws in how we evaluate leaders (from kahnema...                   \n",
       "497          intel unveils oversized novelty processor                   \n",
       "498  ariana grande singlehandedly saves tidal with ...                   \n",
       "\n",
       "[499 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "adj_data = pd.read_table(\"./AP2/adjudicated_data.txt\")\n",
    "adj_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 100, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(adj_data, test_size=100)\n",
    "train, dev = train_test_split(train, test_size=100)\n",
    "len(train), len(dev), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"./splits/train.txt\", sep=\"\\t\", index=False)\n",
    "dev.to_csv(\"./splits/dev.txt\", sep=\"\\t\", index=False)\n",
    "test.to_csv(\"./splits/test.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for best dev model: 0.475, 95% CIs: [0.378 0.573]\n",
      "\n",
      "humor\t0.900\twoman\n",
      "humor\t0.830\tshe\n",
      "humor\t0.828\tstudy\n",
      "humor\t0.782\tman\n",
      "humor\t0.750\tteacher\n",
      "humor\t0.719\tgod\n",
      "humor\t0.707\tdressing\n",
      "humor\t0.697\tlike\n",
      "humor\t0.689\troad\n",
      "humor\t0.668\tfucking\n",
      "\n",
      "irony\t1.138\thero\n",
      "irony\t0.926\tsleep\n",
      "irony\t0.791\tnetflix\n",
      "irony\t0.765\twishes\n",
      "irony\t0.712\tup\n",
      "irony\t0.697\tfloor\n",
      "irony\t0.610\tintroduces\n",
      "irony\t0.565\thow\n",
      "irony\t0.562\thave\n",
      "irony\t0.534\tplace\n",
      "\n",
      "none\t0.954\twe\n",
      "none\t0.829\ta\n",
      "none\t0.766\t.\n",
      "none\t0.748\tmississippi\n",
      "none\t0.724\ttwitter\n",
      "none\t0.670\tgop\n",
      "none\t0.624\tflorida\n",
      "none\t0.617\tus\n",
      "none\t0.590\thomeless\n",
      "none\t0.584\twest\n",
      "\n",
      "satire\t0.904\tnow\n",
      "satire\t0.903\treally\n",
      "satire\t0.754\tabout\n",
      "satire\t0.751\tu.s.\n",
      "satire\t0.716\tfeatures\n",
      "satire\t0.706\tdoctors\n",
      "satire\t0.697\tconscious\n",
      "satire\t0.695\t$\n",
      "satire\t0.673\tsentenced\n",
      "satire\t0.672\tschool\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingFile = \"splits/train.txt\"\n",
    "devFile = \"splits/dev.txt\"\n",
    "testFile = \"splits/test.txt\"\n",
    "    \n",
    "run(trainingFile, devFile, testFile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving on the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW2",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
